#fullstack #gpt #Nomad_Coder [[Fullstack GPT]]

## 5.0 ConversationBufferMemory
### 1. Conversation Buffer Memory
- 대화의 모든 내용을 문자열로 저장하는 방식
- 오래된 대화까지 모두 그대로 담기 때문에 메모리 사용량에 주의
```python
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory()

# 메모리에 정해진 대화 내용을 저장하는 함수 (모든 메모리 API에서 동일)
memory.save_context({"input": "hi"}, {"output": "How are you?"})

# 메모리에 저장된 대화들을 불러오는 함수 (모든 메모리 API에서 동일)
memory.load_memory_variables({})

'''
{'history': 'Human: hi\nAI: How are you?'}
'''
```

```python
# return_message=True: 대화 내용을 리스트에 담아서 보여줌
memory = ConversationBufferMemory(return_messages=True)  

memory.save_context({"input": "hi"}, {"output": "How are you?"})

memory.load_memory_variables({})

'''
{'history': [HumanMessage(content='hi'), AIMessage(content='How are you?')]}
'''
```



## 5.1 ConversationBufferWindowMemory
### 1. Conversation Buffer Window Memory
- Window의 크기만큼만의 최신 대화 내용을 저장
- window size를 넘어가는 메세지가 들어오면 제일 오래된 메세지가 탈락
- 장점: 버퍼가 너무 커지지 않도록 관리
- 단점: 긴 대화의 흐름이 필요한 경우 이전 대화 기록이 없어질 수 있음

```python
from langchain.memory import ConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(return_messages=True, k=3) # k: window size

def add_message(input, output):
    memory.save_context({'input': input}, {'output': output})

add_message(1, 1)
add_message(2, 2)
add_message(3, 3)

memory.load_memory_variables({})

'''
{'history': [HumanMessage(content='1'),
  AIMessage(content='1'),
  HumanMessage(content='2'),
  AIMessage(content='2'),
  HumanMessage(content='3'),
  AIMessage(content='3')]}
'''
```

```python
# k값보다 더 많이 메세지를 넣을 경우

add_message(4, 4)
memory.load_memory_variables({})

'''
{'history': [HumanMessage(content='2'),
  AIMessage(content='2'),
  HumanMessage(content='3'),
  AIMessage(content='3'),
  HumanMessage(content='4'),
  AIMessage(content='4')]}
'''
```



## 5.2 ConversationSummaryMemory
### 1. Conversation Summary Memory
- 대화의 긴 맥락을 요약하여 유지하는 메모리 방식


### 2. 작동 방식
1. 대화가 진행될수록 메모리가 쌓이는데, 이전 메시지들의 주요 내용을 요약(summary)하여 저장
2. 요약된 정보를 바탕으로 대화의 흐름을 이해하거나 관련성을 유지
3. 메모리 크기를 줄이고 모델의 처리 부담을 줄이는 데 유용


### 3. 특징
- 효율성: 이전 대화 전체가 아닌 요약본만 남기기 때문에 대규모 대화에도 메모리 사용량이 적음
- 대화 맥락 유지: 주요 정보를 축약하여 계속 참조하기 때문에 맥락을 유지 가능
- 장기 대화 적합성: 대화를 길게 이어갈수록 요약을 반복 갱신하면서도 핵심 맥락을 유지

```python
from langchain.memory import ConversationSummaryMemory
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(temperature=0.1)

memory = ConversationSummaryMemory(llm=llm)

def add_message(input, output):
    memory.save_context({"input": input}, {"output": output})

def get_history():
    return memory.load_memory_variables({})

add_message("Hi I'm Suyoung, I live in South Korea.", "This is cool!")
add_message("South Korea is so pretty!", "I wish I could go!")

get_history()

---
# output
{
	'history': 'Suyoung from South Korea introduces themselves to the AI, who responds positively. The human comments on the beauty of South Korea, and the AI expresses a desire to visit.'
}
```



## 5.3 ConversationSummaryBufferMemory
### 1. Conversation Summary Buffer Memory
- 요약과 버퍼를 결합하여 대화 내용을 저장하고 참조하는 방식
- 요약된 정보 뿐 아니라 최신 대화 내용을 일정 기간(버퍼 크기) 동안 함께 보존


### 2. 작동 방식
1. 대화의 주요 내용을 요약하여 저장
2. 동시에, 최근 몇 개의 메시지를 원본 그대로(버퍼 형태) 저장하여 세부 맥락을 유지
3. 버퍼가 가득 차면 가장 오래된 메시지를 삭제하고 새로운 메시지를 추가


### 3. 특징
- 세부 정보 보존: 요약과 함께 최근 대화의 세부 내용을 유지하므로 세부적인 응답이 가능
- 조화로운 메모리 관리: 오래된 메시지는 요약으로 대체되고, 최근 메시지는 버퍼에 남아 세부 정보를 제공
- 효율성과 맥락 유지의 균형: 요약의 장점과 버퍼의 구체성을 동시에 활용

```python
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(temperature=0.1)

memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=50,  # 이 토큰 수 이상의 메시지는 요약해서 저장 
    return_messages=True,
    buffer_size=3  # 원본 그대로 저장하는 대화의 수
)

def add_message(input, output):
    memory.save_context({"input": input}, {"output": output})

def get_history():
    return memory.load_memory_variables({})

add_message("Hi I'm Nicolas, I live in South Korea", "Wow that is so cool!")
add_message("South Korea is so pretty", "I wish I could go!!")
add_message("How far is Korea from Argentina?", "I don't know! Super far!")
add_message("How far is Brazil from Argentina?", "I don't know! Super far!")
add_message("How far is France from Korea?", "I don't know! Super far!")
add_message("How far is USA from Korea?", "I don't know! Super far!")

get_history()

---
# output
{
	'history': [
		SystemMessage(content="Nicolas from South Korea introduces himself to the AI, who responds with enthusiasm. The conversation shifts to the beauty of South Korea, with the AI expressing a desire to visit. Nicolas asks how far Korea is from Argentina. The AI admits it doesn't know, saying it's super far. The human then asks how far Brazil is from Argentina. The AI responds that it doesn't know, it's super far. When asked how far France is from Korea, the AI is unable to provide an answer.", additional_kwargs={}, response_metadata={}),
		AIMessage(content="I don't know! Super far!", additional_kwargs={}, response_metadata={}),
		HumanMessage(content='How far is USA from Korea?', additional_kwargs={}, response_metadata={}),
		AIMessage(content="I don't know! Super far!", additional_kwargs={}, response_metadata={})
	]
}
```



## 5.4 ConversationKGMemory
### ConversationKGMemory
- Conversation Knowledge Graph Memory의 약자
- 대화 중에 얻은 정보를 지식 그래프 형태로 저장하고 불러오는 역할
- 대화의 맥락을 구조화된 방식으로 유지하고, 축적된 지식을 기반으로 대화를 진행해야 할 때 적합

- 주요 기능
	- 지식 그래프 생성 및 관리: 대화에서 추출한 엔티티와 그들 간의 관계를 지식 그래프로 구성하여 저장 
	- 엔티티 및 관계 추적: 대화 중 등장하는 주요 엔티티와 그들의 속성, 상호 관계를 지속적으로 추적하고 업데이트
	- 구조화된 정보 제공: 저장된 지식 그래프를 활용하여 대화의 맥락을 구조화된 방식으로 이해하고, 이를 기반으로 정확하고 일관된 정보를 제공

- 장점:
	- 복잡한 관계 파악: 대화에서 발생하는 복잡한 엔티티 간의 관계를 시각적으로 표현
	- 지식 기반 대화 유지: 대화 중 축적된 지식을 활용하여 일관성 있고 풍부한 정보를 제공 가능


### Entity
- Entity: 데이터 또는 정보를 표현하는 개별적인 개체
	- ConversationKGMemory에서는 **구체적인 정보 단위**를 말함

- ConversationKGMemory에서의 Entity 사용 방식
	1. 정보 추출 및 저장
	    - "김셜리 씨는 판교에 거주 중입니다."라는 문장에서 엔티티는 `김셜리(사람)`와 `판교(장소)`
	    - 이 정보를 지식 그래프에 저장하면 `김셜리 → 거주지 → 판교`와 같은 관계를 표현 가능
	2. 맥락 유지    
	    - 사용자가 이후 대화에서 "그는 어디에서 일하나요?"라고 질문했을 때, "그"가 `김셜리`임을 이해하고 관련 정보를 제공
	3. 질문 답변    
	    - "김셜리 씨의 직업은 무엇인가요?"라는 질문에서, 시스템은 지식 그래프를 조회해 `신입 디자이너`라는 답을 찾을 수 있음
	4. 엔티티 간 관계 표현    
	    - "김셜리 씨는 삼성전자에서 일하며 서울에 거주합니다."라는 문장은 두 개의 엔티티(`김셜리`와 `삼성전자`)와 두 개의 관계(`일함`, `거주`)를 나타냄


### 사용 예시
```python
from langchain.memory import ConversationKGMemory
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(temperature=0.1)

memory = ConversationKGMemory(
    llm=llm,
    return_messages=True,
)

def add_message(input, output):
    memory.save_context({"input": input}, {"output": output})
    
add_message("Hi I'm Yun, I live in South Korea", "This is so coooool!")

memory.load_memory_variables({"input": "Who is Yun?"})

---
"""
# output
{'history': [SystemMessage(content='On Yun: Yun lives in South Korea.', additional_kwargs={}, response_metadata={})]}
"""
```



## 5.6 Chat Based Memory
### Chat Based Memory
- 대화형 애플리케이션에서 사용자의 컨텍스트를 저장하고 관리하는 데 초점을 둔 메모리 시스템


### 주요 특징
- 대화 컨텍스트 유지
	- 사용자와의 대화 흐름을 기반으로 정보를 저장
	- 이전 메시지나 맥락을 활용해 현재 대화를 개선
- 저장 방식
	- 이전 대화 기록을 순차적으로 저장
	- 필요 시 요약을 통해 오래된 데이터를 간소화



## 5.7 LCEL Based Memory
### LCEL Based Memory
- 데이터의 임베딩 기술을 활용하여 정보를 고차원 벡터 공간에 매핑하고, 문맥적 유사성을 학습하여 효율적인 정보 검색 및 저장이 가능


### 주요 특징
- 임베딩 활용
	- 텍스트, 이미지 등 다양한 유형의 데이터를 벡터로 변환하여 저장
	- 벡터로 저장하기 때문에 유사도 검색 및 문맥적 의미 분석이 가능
- 저장 방식
	- 벡터 데이터베이스를 사용해 정보 검색을 최적화
	- 새로운 데이터는 기존 벡터와 유사성을 비교하며 업데이트되거나 저장됨