#fullstack #gpt #Nomad_Coder [[Fullstack GPT]]

## 8.0 Introduction
### PrivateGPT
- 사용자가 로컬에서 AI 모델을 실행하여 데이터를 처리하는 방식
- **데이터의 프라이버시를 보장**하는 것이 핵심
	- 일반적인 GPT 모델은 클라우드 서버에서 동작하므로, 데이터가 외부 서버로 전송됨
	- PrivateGPT는 모든 연산이 사용자의 로컬 환경에서 실행되기 때문에, 민감한 데이터가 외부로 유출되지 않음


### PrivateGPT를 만드는 방법
1. 로컬 환경 준비
	- 로컬 머신에 필요한 AI 모델을 다운로드
	- AI 모델 실행을 위한 라이브러리 설치 (`transformers`, `langchain` 등)
2. 지원되는 AI 모델 선택
	- `GPT4All`, `LLaMA`, `Mistral` 등의 오픈소스 AI 모델 사용 가능
3. PrivateGPT 구현
	- 모델 파일과 데이터를 로컬에 배치한 후 PrivateGPT를 실행
4. 로컬 데이터 인덱싱
	- PDF, 텍스트 파일 등 로컬 데이터에 대해 AI 모델이 학습할 수 있도록 인덱싱 작업

---
## 8.1 HuggingFaceEndpoint
### HuggingFace Hub
1. 개념
	- 모델, 데이터셋, 토크나이저 등을 저장하고 공유하는 플랫폼
	- 머신러닝 모델 및 리소스를 업로드하거나 다운로드하여 사용할 수 있는 저장소 역할
2. 주요 특징
	- 모델 저장 및 공유
	    - 머신러닝 모델을 저장하고, 다른 사용자와 공유 가능
	    - 오픈소스 모델뿐만 아니라 비공개 모델도 관리 가능
	- 커뮤니티
	    - 모델 페이지에는 사용 설명서, 데모, 관련 코드, 토론 섹션이 포함되어 있어 사용자들이 협업 가능
	- Hub에 연결된 라이브러리
	    - `transformers`, `datasets`, `accelerate` 등의 라이브러리와 직접 통합되어 사용이 간편


### HuggingFace Endpoint
1. 개념
	- Hugging Face Inference API를 통해 모델을 직접 호출하여 사용할 수 있는 기능
	- 로컬에서 모델을 실행할 필요 없이, 클라우드에서 호스팅된 모델을 통해 추론을 수행
2. 주요 특징
	- Inference API
	    - Hugging Face에서 제공하는 REST API를 통해 모델을 호출
	    - 클라우드 상에서 실행되므로, 로컬 머신에 모델을 다운로드하거나 환경을 설정할 필요가 없음
	- 자동 확장성
	    - Hugging Face Endpoint는 클라우드 인프라를 기반으로 하며, 대규모 요청 처리 시 자동으로 확장
	- 사용의 편리함
	    - EST API를 통해 쉽게 통합 가능하며, Python, JavaScript 등 다양한 언어에서 호출 가능


### Hugging Face Hub vs Hugging Face Endpoint

| 특징    | Hugging Face Hub                | Hugging Face Endpoint          |
| ----- | ------------------------------- | ------------------------------ |
| 역할    | 모델/데이터셋/코드 저장소                  | 모델을 API로 호출하여 추론 수행            |
| 호스팅   | 로컬 또는 클라우드 환경에서 모델 다운로드 후 실행 필요 | Hugging Face의 클라우드 인프라에서 바로 실행 |
| 추론 방식 | 사용자가 모델을 로컬로 로드하여 실행            | REST API를 통해 클라우드에서 추론 결과 제공   |
| 사용 목적 | 모델 개발 및 연구                      | 빠르고 간편한 모델 추론 및 API 통합         |
| 확장성   | 모델 실행은 사용자의 리소스에 의존             | 클라우드 기반으로 자동 확장 지원             |
| 요금제   | 무료/유료 모델 혼합, 로컬에서 실행 시 추가 요금 없음 | API 호출량에 따라 사용 요금 부과           |


### 사용 예시
```python
from langchain.llms.huggingface_endpoint import HuggingFaceEndpoint
from langchain.prompts import PromptTemplate

# HuggingFace 모델 선택
llm = HuggingFaceEndpoint(
    repo_id="mistralai/Mistral-7B-Instruct-v0.3",  # 선택한 모델의 ID
    temperature=0.5,
    huggingfacehub_api_token=HUGGINGFACEHUB_API_KEY,
)

prompt = PromptTemplate.from_template("What is the meaning of {word}?")

chain = prompt | llm
result = chain.invoke({"word": "potato"})
print(result)
```

---
## 8.2 HuggingFacePipeline 
### HuggigFace Pipeline
- Hugging Face 모델을 로컬에서 실행할 수 있도록 해주는 클래스
- Hugging Face의 `transformers` 라이브러리를 통해 로컬 컴퓨터에서 모델을 실행


### 특징
- 모델을 로컬 머신에 다운로드하여 실행
- python 코드로 간단히 사용 가능
- 네트워크 요청 없이 작동하여 클라우드 비용 없이 무료로 사용 가능
- 데이터 보안 측면에서 유리
- 로컬 컴퓨터의 성능 혹은 GPU의 여부로 인해 모델 실행의 속도가 달라짐


### Hugging Face Endpoint vs Hugging Face Pipeline
| 특징    | Hugging Face Endpoint          | Hugging Face Pipeline          |
| ----- | ------------------------------ | ------------------------------ |
| 역할    | 모델을 API로 호출하여 추론 수행            | 로컬 환경에서 모델을 로드하여 직접 추론 수행      |
| 호스팅   | Hugging Face의 클라우드 인프라에서 바로 실행 | 사용자의 로컬 컴퓨터에서 실행               |
| 추론 방식 | REST API를 통해 클라우드에서 추론 결과 제공   | 로컬 메모리에 모델을 로드하여 python 코드로 실행 |
| 사용 목적 | 빠르고 간편한 모델 추론 및 API 통합         | 소규모 애플리케이션, 실험적 사용, 오프라인 추론    |
| 확장성   | 클라우드 기반으로 자동 확장 지원             | 로컬 리소스에 의존, 확장이 어려움            |
| 요금제   | API 호출량에 따라 사용 요금 부과           | 로컬에서 실행 시 무료, 추가 리소스 비용 없음     |


### 사용 순서
#### Model Download
```python
%pip install --upgrade --quiet  transformers --quiet
```

```python
# 허깅페이스 모델/토크나이저를 다운로드 받을 경로
# (예시)
import os

# ./cache/ 경로에 다운로드 받도록 설정
os.environ["TRANSFORMERS_CACHE"] = "./cache/"
os.environ["HF_HOME"] = "./cache/"
```

#### Model Loading
```python
from langchain.llms.huggingface_pipeline import HuggingFacePipeline
from langchain.prompts import PromptTemplate

# HuggingFace 모델을 다운로드
llm = HuggingFacePipeline.from_model_id(
    model_id="openai-community/gpt2",  # 사용할 모델의 ID
    task="text-generation",  # 수행할 작업 지정
    device=0,  # 0: GPU, -1: CPU (default)
    
    # 파이프라인에 전달할 추가 인자 설정 
    pipeline_kwargs={
        "max_new_tokens":50
    }
)

prompt = PromptTemplate.from_template("A {word} is a")

chain = prompt | llm
result = chain.invoke({"word": "banana"})
print(result)
```

- `from_model_id()`메서드를 사용하여 모델 매개변수를 지정함으로써 모델을 로드
> 


---
## 8.3 GPT4All

---
## 8.4 Ollama 

---
## 8.5 Conclusions 
